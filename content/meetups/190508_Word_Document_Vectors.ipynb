{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeX_Qngx5fch",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word & Document Vectors\n",
    "\n",
    "Huntsville AI - May 8, 2019\n",
    "\n",
    "Facebook: Huntsville Ai\n",
    "\n",
    "LinkedIn: Huntsville AI\n",
    "\n",
    "GitHub: HSV-AI\n",
    "\n",
    "Mailing List - send an e-mail to jlangley@sessionboard.com to be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0MeNkMb6dk_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Word_embedding): \n",
    "\n",
    "> Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.\n",
    "\n",
    "\n",
    "The main function is to create a vector of numbers that represent a word based on the context in which that word is used. These vectors can then be used in a relative fashion to determine the relatedness of words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adWqxBR5zi5I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Papers\n",
    "\n",
    "Here is a list of the seminal papers that led to the capability available today for word and document vectors:\n",
    "\n",
    "* **2013** - *Distributed Representations of Words and Phrases and their Compositionality*\n",
    "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean -  https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "\n",
    "* **2013** - *Efficient Estimation of Word Representations in Vector Space* Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean - https://arxiv.org/pdf/1301.3781.pdf\n",
    "\n",
    "* **2014** - *Distributed Representations of Sentences and Documents*\n",
    "Quoc V. Le, Tomas Mikolov - https://arxiv.org/pdf/1405.4053\n",
    "\n",
    "* **2015** - *From Word Embeddings To Document Distances* Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger - http://proceedings.mlr.press/v37/kusnerb15.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVtVk1Q45fct",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Vectors\n",
    "\n",
    "In order to compute the word vectors, we create a neural network and train it to predict things based on either a Skip-gram or Continuous Bag of Words approach. The weights of the hidden layer then become the values used in the word vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DyrZxqLv5fcw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![alt text](https://cdn-images-1.medium.com/max/1000/0*DY41kNV4X5j_PfXA.png)\n",
    "\n",
    "Image from [Efficient Estimation of Word Representations in\n",
    "Vector Space](https://arxiv.org/pdf/1301.3781.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Poos8Og9Bwyd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Word2Vec](https://multithreaded.stitchfix.com/assets/posts/2016-05-27-lda2vec/anim00.gif)\n",
    "\n",
    "Image from [StitchFix Blog](https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rM4LkRla5fc3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document Vectors\n",
    "\n",
    "Document Vectors are created by adding an additional document (or paragraph) ID as an input.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/800/0*x-gtU4UlO8FAsRvL.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RUjUhtZ-pH1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![alt text](https://cdn-images-1.medium.com/max/800/0*NtIsrbd4VQzUKVKr.)\n",
    "\n",
    "\n",
    "Images from [A gentle introduction to Doc2Vec\n",
    "](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyOhFDXp5fdA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Word Vector Image](https://www.tensorflow.org/images/linear-relationships.png)\n",
    "\n",
    "\n",
    "Image from [Vector Representations of Words](https://www.tensorflow.org/tutorials/representation/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1C0elSC-_alg",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Cosine Similarity](https://cdn-images-1.medium.com/max/800/0*XMW5mf81LSHodnTi.png)\n",
    "\n",
    "\n",
    "Image from [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-SwW3wC5fdE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Movers Distance\n",
    "\n",
    "The distance between words of different sentences can be used to judge the similarity of the sentences or documents.\n",
    "\n",
    "![Word Movers Distance](https://cdn-images-1.medium.com/max/800/1*nTWAm46JMYWXpHVsS9MA5w.png)\n",
    "\n",
    "Image from [From Word Embeddings To Document Distances](https://towardsdatascience.com/word-distance-between-word-embeddings-cc3e9cf1d632)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2LFFlyYzcxu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "https://github.com/stanfordnlp/GloVe\n",
    "\n",
    "https://github.com/fastai/word-embeddings-workshop\n",
    "\n",
    "https://towardsdatascience.com/lda2vec-word-embeddings-in-topic-models-4ee3fc4b2843\n",
    "\n",
    "https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec\n",
    "\n",
    "https://stackoverflow.com/questions/38287772/cbow-v-s-skip-gram-why-invert-context-and-target-words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7V-HwkC5fdI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Application\n",
    "\n",
    "Below, we will walk through some examples from the Fast.ai Word Embeddings Workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "Q5Fkvsu95fdM",
    "outputId": "61fa8076-e772-419e-ee3e-0332a3f68ea3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-08 18:34:26--  http://files.fast.ai/models/glove_50_glove_100.tgz\n",
      "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
      "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 225083583 (215M) [text/plain]\n",
      "Saving to: ‘glove_50_glove_100.tgz.1’\n",
      "\n",
      "glove_50_glove_100. 100%[===================>] 214.66M   107MB/s    in 2.0s    \n",
      "\n",
      "2019-05-08 18:34:28 (107 MB/s) - ‘glove_50_glove_100.tgz.1’ saved [225083583/225083583]\n",
      "\n",
      "glove_vectors_100d.npy\n",
      "glove_vectors_50d.npy\n",
      "words.txt\n",
      "wordsidx.txt\n"
     ]
    }
   ],
   "source": [
    "#Get the data and untar it\n",
    "\n",
    "!wget http://files.fast.ai/models/glove_50_glove_100.tgz \n",
    "\n",
    "!tar xvzf glove_50_glove_100.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nVjBvdaa5fdZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#import packages needed\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEFXEfWo5fdg",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will load the glove 50 & 100 vectors as numpy arrays and load the words and word indices as arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EID7Pek5fdi",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vecs = np.load(\"glove_vectors_100d.npy\")\n",
    "vecs50 = np.load(\"glove_vectors_50d.npy\")\n",
    "\n",
    "with open('words.txt') as f:\n",
    "    content = f.readlines()\n",
    "words = [x.strip() for x in content]\n",
    "\n",
    "wordidx = json.load(open('wordsidx.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xzsvbp9B5fdn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's see what this data looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iimbE8Q45fdp",
    "outputId": "8958adf8-f693-4bb4-cb9f-fa4ce42c5b2a",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UKZBypVx5fdy",
    "outputId": "98b62784-d3b0-4152-8dd7-dbfbdaf0a897",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AoJAGdOT5fd6",
    "outputId": "752ca462-7b23-4b19-e5f4-83579e872c19",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4493"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordidx['architect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TP1Fvr5Y5feH",
    "outputId": "021c1fc3-7499-496a-bd4b-87c6c12e7471",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'architect'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[4493]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9qrkDle5feR",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What about that word vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "2deAH5VY5feW",
    "outputId": "c9d0b9f7-90eb-4196-b652-89009c16f6b6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2675, -0.1654, -0.6296,  0.448 ,  0.452 , -0.6424,  0.042 ,\n",
       "       -1.0137, -0.6596,  0.9818,  0.2998, -0.7044,  0.0481, -0.3909,\n",
       "       -0.2515, -0.0907,  0.5111,  0.2321, -1.3972,  0.0896, -1.0962,\n",
       "        0.1159,  0.0979, -0.7837,  0.1524, -1.3648, -0.5557, -1.0818,\n",
       "       -0.2341, -0.6261, -0.8803,  0.536 ,  0.1439,  0.335 , -0.4361,\n",
       "       -0.0788,  0.2288, -0.4465,  0.6148, -0.2139,  0.4312,  0.1618,\n",
       "        1.0763,  0.4359,  0.4286, -0.3155, -0.0784, -0.5784, -0.1905,\n",
       "        0.1904, -0.1977, -0.5946,  0.6593,  0.2798, -0.0671, -1.6904,\n",
       "       -0.9657,  0.044 ,  0.3146, -0.491 ,  0.3345,  0.3266,  0.3003,\n",
       "        0.4409,  0.7353, -0.599 ,  0.1626,  1.012 , -0.3043, -0.1179,\n",
       "       -0.3546,  0.6402, -0.8409, -0.3581,  0.1925, -1.1535,  0.6362,\n",
       "        0.8889, -0.0116, -0.2549,  0.3039,  0.2562, -0.0331,  0.4997,\n",
       "       -0.0159,  0.3529, -0.2008, -0.5076, -0.4175, -1.4415,  0.7295,\n",
       "       -0.8933,  0.5672,  0.607 ,  0.0374,  0.0441, -0.2491, -1.014 ,\n",
       "        0.0384, -0.5015], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[4493]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5o1uSFKR5fed",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHz7oSVr5fem",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Smaller numbers mean two words are closer together, larger numbers mean they are further apart.\n",
    "\n",
    "The distance between similar words is low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "27BHu_Du5feo",
    "outputId": "0bfa06ce-e642-467f-91e6-bb994673c28e",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27636247873306274"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(vecs[wordidx[\"puppy\"]], vecs[wordidx[\"dog\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJ_eP0Bi5fe0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And the distance between unrelated words is high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jH25sLXy5ffD",
    "outputId": "9215cdb4-7881-4be1-feef-053ffe8ed2f4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9621107056736946"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(vecs[wordidx[\"avalanche\"]], vecs[wordidx[\"antique\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQ9ldR1d5ffN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Bias\n",
    "\n",
    "The word vectors will pick up any bias that exists in the data used to build the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "83AJxPJ15ffT",
    "outputId": "560ee69b-6f22-4d3f-d0c0-e218ed56550b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5098515152931213"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(vecs[wordidx[\"man\"]], vecs[wordidx[\"genius\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pf6jNXdF5fff",
    "outputId": "73dd7703-46c4-4e02-e299-337977700e58",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.689783364534378"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(vecs[wordidx[\"woman\"]], vecs[wordidx[\"genius\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nikczm75ffq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Nearest Neighbors\n",
    "\n",
    "We can also see what words are close to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "kT2WaK8F5fft",
    "outputId": "f72b4e71-b96d-4d9f-d00e-764734eb754d",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('antique', 1.1920929e-07),\n",
       " ('antiques', 0.18471009),\n",
       " ('furniture', 0.2613591),\n",
       " ('jewelry', 0.26212162),\n",
       " ('vintage', 0.28011894),\n",
       " ('handmade', 0.32542467),\n",
       " ('furnishings', 0.3287084),\n",
       " ('reproductions', 0.33931458),\n",
       " ('decorative', 0.35905504),\n",
       " ('pottery', 0.3720798)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=10, radius=0.5, metric='cosine', algorithm='brute')\n",
    "neigh.fit(vecs)\n",
    "\n",
    "distances, indices = neigh.kneighbors([vecs[wordidx[\"antique\"]]])\n",
    "\n",
    "[(words[int(ind)], dist) for ind, dist in zip(list(indices[0]), list(distances[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJmEee_i5ff1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Math with Word Vectors\n",
    "\n",
    "You can do some pretty interesting things with these word vectors. We can combine multiple terms and use them as a single input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "h_5_0Ke05ff3",
    "outputId": "79723967-7751-43e5-8a12-d6abbb35d5f5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0345 -0.1185  0.746   0.3256  0.3256 -1.4699 -0.8715 -0.9421  0.0679\n",
      "  0.922   0.6811 -0.3729  1.0969  0.7196  1.3515  1.2493  0.6621  0.1901\n",
      " -0.2707 -0.0444 -1.232   0.1744  0.7577 -0.9177 -1.2184  0.6959 -0.1966\n",
      " -0.415  -0.3358  0.5452  0.589  -0.0299 -0.9744 -0.8937  0.2283 -0.2092\n",
      " -1.3795  1.7811  0.2269  0.47   -0.3045 -0.1573 -0.478   0.3071  0.4202\n",
      " -0.4434  0.1602  0.1443 -0.9528 -0.5565  0.7537  0.182   1.4008  1.8967\n",
      "  0.595  -3.0072  0.6811 -0.2557  2.0217  0.7825  0.4251  1.3615  0.5902\n",
      " -0.1312  0.9344 -0.5377 -0.3988 -0.6415  0.6527  0.5117  0.7315  0.1396\n",
      "  0.3785 -0.6403 -0.094   0.1076  0.6197  0.2537 -1.4346  1.169   1.6931\n",
      "  0.1458 -0.5981  0.8195 -3.1903  1.2429  2.1481  1.6004  0.2014 -0.2121\n",
      "  0.3698 -0.001  -0.628   0.2869  0.3119 -0.1093 -0.6341 -1.7804  0.5857\n",
      "  0.3702]\n"
     ]
    }
   ],
   "source": [
    "new_vec = vecs[wordidx[\"artificial\"]] + vecs[wordidx[\"intelligence\"]]\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "3RYWDLAa5fgA",
    "outputId": "75689f22-5d2a-485d-8584-7b0aa723aa12",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('intelligence', 0.1883161),\n",
       " ('artificial', 0.25617576),\n",
       " ('information', 0.3256532),\n",
       " ('knowledge', 0.336419),\n",
       " ('secret', 0.36480355),\n",
       " ('human', 0.36726683),\n",
       " ('biological', 0.37090683),\n",
       " ('using', 0.3773631),\n",
       " ('scientific', 0.38513905),\n",
       " ('communication', 0.3869152)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, indices = neigh.kneighbors([new_vec])\n",
    "\n",
    "[(words[int(ind)], dist) for ind, dist in zip(list(indices[0]), list(distances[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdEFwrPd5fgI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can even move from one place to another in the vector space. Beware of bias taking you in unintended directions though. Here's the general sense of the word \"programmer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "135SSvuz5fgK",
    "outputId": "0fd43315-4ec0-4ad7-cc9d-7f0a58120c9b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programmer', 0.0),\n",
       " ('programmers', 0.32259798),\n",
       " ('animator', 0.36951017),\n",
       " ('software', 0.38250887),\n",
       " ('computer', 0.40600342),\n",
       " ('technician', 0.41406858),\n",
       " ('engineer', 0.4303757),\n",
       " ('user', 0.4356534),\n",
       " ('translator', 0.43721014),\n",
       " ('linguist', 0.44948018)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, indices = neigh.kneighbors([vecs[wordidx[\"programmer\"]]])\n",
    "[(words[int(ind)], dist) for ind, dist in zip(list(indices[0]), list(distances[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QGFzbAvS5fgV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's the masculine sense of the word \"programmer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Jh6pTdt35fgY",
    "outputId": "e406037c-a289-4d4b-e826-570ac772f638",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programmer', 0.17419636),\n",
       " ('programmers', 0.4133587),\n",
       " ('engineer', 0.46376407),\n",
       " ('compiler', 0.46731704),\n",
       " ('software', 0.4681465),\n",
       " ('animator', 0.4892366),\n",
       " ('computer', 0.5046158),\n",
       " ('mechanic', 0.5150067),\n",
       " ('setup', 0.51882535),\n",
       " ('developer', 0.51953185)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vec = vecs[wordidx[\"programmer\"]] + vecs[wordidx[\"he\"]] - vecs[wordidx[\"she\"]]\n",
    "distances, indices = neigh.kneighbors([new_vec])\n",
    "[(words[int(ind)], dist) for ind, dist in zip(list(indices[0]), list(distances[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mg3cLWiA5fgg",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's the feminine sense of the word \"programmer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "UuvxxuZ-5fgi",
    "outputId": "1068a1b4-fd2c-4f81-f5e7-089fb12c7d83",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programmer', 0.19503415),\n",
       " ('stylist', 0.42715955),\n",
       " ('animator', 0.4820645),\n",
       " ('programmers', 0.48337305),\n",
       " ('choreographer', 0.4862678),\n",
       " ('technician', 0.4862805),\n",
       " ('designer', 0.48710012),\n",
       " ('prodigy', 0.49118334),\n",
       " ('lets', 0.49730027),\n",
       " ('screenwriter', 0.49754214)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vec = vecs[wordidx[\"programmer\"]] - vecs[wordidx[\"he\"]] + vecs[wordidx[\"she\"]]\n",
    "distances, indices = neigh.kneighbors([new_vec])\n",
    "[(words[int(ind)], dist) for ind, dist in zip(list(indices[0]), list(distances[0]))]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "190508_Word_Document_Vectors.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
